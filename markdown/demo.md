# 大语言模型 (LLM) 基础知识

本文档整理了有关大语言模型（Large Language Models, LLMs）的核心概念、训练过程及应用模式，旨在为 RAG（检索增强生成）系统提供基础知识库。

## 1. 什么是大语言模型 (LLM)？

大语言模型是基于深度学习算法（主要是 Transformer 架构）构建的人工智能模型，通过在海量文本数据上进行训练，学习语言的统计规律和语义关联。它们不仅能理解自然语言，还能进行文本生成、翻译、摘要、逻辑推理甚至编写代码。

**核心特征：**
*   **规模大**：参数量通常在数十亿（B）到数万亿（T）级别。
*   **通用性**：不局限于单一任务，具备“涌现”能力（Emergent Abilities），即在规模扩大后表现出训练时未明确教过的能力。

## 2. 核心技术概念

### 2.1 Transformer 架构
现代 LLM 的基石。其核心是**自注意力机制 (Self-Attention)**，使模型在处理每个词时，都能同时关注到上下文中的其他词，从而捕捉长距离的语义依赖关系。

### 2.2 Token (词元)
模型处理文本的基本单位。
*   一个 Token 可以是一个单词、一个字符或单词的一部分。
*   **估算**：通常 1000 个 Token 约等于 750 个英文单词或 400-500 个汉字。

### 2.3 Context Window (上下文窗口)
模型一次能处理的最大 Token 数量（包括输入和输出）。
*   窗口越大，模型能“记住”的对话历史或处理的文档越长。
*   例如：Claude 3 拥有 200k 的上下文窗口，可一次处理整本书。

### 2.4 Parameters (参数)
模型内部的权重，类似于神经元之间的连接强度。参数越多，模型的“脑容量”和表达能力通常越强，但计算资源消耗也越大。

## 3. LLM 是如何炼成的？ (训练流水线)

LLM 的诞生通常经历三个阶段：

1.  **预训练 (Pre-training)**
    *   **目标**：预测下一个 Token (Next Token Prediction)。
    *   **数据**：海量互联网文本（书籍、网页、代码）。
    *   **结果**：获得一个具备丰富世界知识但不懂指令的“基座模型” (Base Model)。

2.  **有监督微调 (SFT, Supervised Fine-Tuning)**
    *   **目标**：学会遵循指令。
    *   **数据**：高质量的问答对（Prompt-Response）。
    *   **结果**：模型变成能回答问题、执行任务的助手 (Chat Model)。

3.  **人类反馈强化学习 (RLHF, Reinforcement Learning from Human Feedback)**
    *   **目标**：对齐人类价值观（有用性、安全性、真实性）。
    *   **方法**：让模型生成多个回答，由人类排序，训练奖励模型来优化 LLM。
    *   **结果**：回答更符合人类偏好，减少有害输出。

## 4. 关键应用模式

### 4.1 Prompt Engineering (提示工程)
通过设计高质量的输入文本（Prompt）来引导模型输出预期结果的技术。
*   **Zero-shot (零样本)**：直接提问，不给示例。
*   **Few-shot (少样本)**：提供几个示例（Example）让模型模仿。
*   **Chain of Thought (CoT, 思维链)**：要求模型“一步步思考”，能显著提升逻辑推理能力。

### 4.2 RAG (检索增强生成)
解决 LLM **幻觉 (Hallucination)** 和 **知识过时** 问题的关键技术。
*   **原理**：
    1.  **Retrieve (检索)**：用户提问时，先在外部知识库（向量数据库）中搜索相关信息。
    2.  **Augment (增强)**：将检索到的信息作为上下文拼接到 Prompt 中。
    3.  **Generate (生成)**：模型根据检索到的事实生成答案。

### 4.3 Agents (智能体)
赋予 LLM 使用工具（如搜索、计算器、API）和规划任务的能力，使其从“对话者”变成“行动者”。

## 5. 常见术语速查

| 术语 | 解释 |
| :--- | :--- |
| **Temperature (温度)** | 控制输出的随机性。数值越高（如 0.8），回答越有创造性但越不稳定；数值越低（如 0.2），回答越保守确定。 |
| **Top-P (核采样)** | 另一种控制随机性的参数，通常与 Temperature 二选一使用。 |
| **Hallucination (幻觉)** | 模型一本正经地胡说八道，生成看似合理但实际错误的内容。 |
| **Embedding (嵌入)** | 将文本转化为向量（数字列表）的过程，用于计算文本之间的相似度，是 RAG 的基础。 |
